import os
import numpy as np
import onnxruntime as ort
import librosa
import soundfile as sf
from transformers import WhisperProcessor

# ---------- CONFIG ----------
ENCODER_ONNX = "encoder.onnx"   # path to encoder .onnx
DECODER_ONNX = "decoder.onnx"   # path to decoder .onnx
AUDIO_FILE = "audio.wav"        # path to audio to transcribe
SAMPLE_RATE = 16000
MAX_OUTPUT_TOKENS = 448        # change as needed
DEVICE = "cpu"                 # "cpu" or "cuda"
# ----------------------------

# load HuggingFace processor (feature extractor + tokenizer)
# this downloads tokenizer/feature_extractor from HF the first time
processor = WhisperProcessor.from_pretrained("openai/whisper-small")

# load audio and produce the model input features (log-mel)
def load_audio_and_extract_features(path):
    audio, sr = librosa.load(path, sr=SAMPLE_RATE, mono=True)
    # WhisperProcessor.feature_extractor returns dict with "input_features"
    feats = processor.feature_extractor(audio, sampling_rate=SAMPLE_RATE, return_tensors="np").input_features
    # feats shape: (batch, seq_len, feature_dim) e.g. (1, T, 80)
    return feats.astype(np.float32)

# create ONNX runtime session with preferred providers
def make_session(path, device="cpu"):
    providers = ["CPUExecutionProvider"]
    if device == "cuda":
        # ONNX runtime GPU provider name may vary; typical: "CUDAExecutionProvider"
        providers = ["CUDAExecutionProvider", "CPUExecutionProvider"]
    sess = ort.InferenceSession(path, providers=providers)
    return sess

# helper to get input name by guess (first input if uncertain)
def first_input_name(session):
    return session.get_inputs()[0].name

# helper to get decoder input and output metadata
def inspect_decoder_io(sess):
    inputs = sess.get_inputs()
    outputs = sess.get_outputs()
    input_names = [i.name for i in inputs]
    output_names = [o.name for o in outputs]
    return input_names, output_names, inputs, outputs

# main pipeline
def run_pipeline(encoder_path, decoder_path, audio_path, device="cpu"):
    # prepare features
    input_feats = load_audio_and_extract_features(audio_path)  # shape (1, T, feat)
    batch_size = input_feats.shape[0]

    # create sessions
    enc_sess = make_session(encoder_path, device=device)
    dec_sess = make_session(decoder_path, device=device)

    # inspect decoder IO to see if kv cache is used
    dec_input_names, dec_output_names, dec_inputs_meta, dec_outputs_meta = inspect_decoder_io(dec_sess)
    print("Decoder input names:", dec_input_names)
    print("Decoder output names:", dec_output_names)

    # run encoder
    # choose encoder input name dynamically
    enc_in_name = first_input_name(enc_sess)
    enc_out = enc_sess.run(None, {enc_in_name: input_feats})
    # assume encoder returns hidden states as first output
    encoder_hidden_states = enc_out[0].astype(np.float32)
    print("Encoder output shape:", encoder_hidden_states.shape)

    # get tokenizer
    tokenizer = processor.tokenizer

    # Prepare greedy decoding
    # start token (depends on tokenizer; use decoder_start_token_id if available)
    if hasattr(tokenizer, "bos_token_id") and tokenizer.bos_token_id is not None:
        start_token_id = tokenizer.bos_token_id
    elif hasattr(tokenizer, "decoder_start_token_id"):
        start_token_id = tokenizer.decoder_start_token_id
    else:
        # fallback: whisper uses 1 or 50257 in different versions; better to read from tokenizer
        start_token_id = tokenizer.convert_tokens_to_ids(tokenizer.all_special_tokens[0])
    print("start_token_id:", start_token_id)

    # Utility to check if any decoder input name indicates 'past' or 'cache'
    uses_past_kv = any(("past" in n.lower() or "cache" in n.lower() or "key" in n.lower() or "kv" in n.lower()) for n in dec_input_names)
    print("Decoder appears to use KV/past cache:", uses_past_kv)

    if not uses_past_kv:
        # Simple slow autoregressive approach: feed the full input_ids each step
        generated = [start_token_id]
        for step in range(MAX_OUTPUT_TOKENS):
            input_ids = np.array([generated], dtype=np.int64)  # shape (1, seq_len)
            feed = {}
            # map decoder inputs heuristically
            for inp in dec_inputs_meta:
                nm = inp.name
                if "input_ids" in nm or "input" in nm and inp.type.endswith("tensor(int64)"):
                    feed[nm] = input_ids
                elif "encoder" in nm or "hidden" in nm or "encoder_hidden_states" in nm:
                    feed[nm] = encoder_hidden_states
                else:
                    # skip others or try to pass zeros
                    pass

            outs = dec_sess.run(None, feed)
            # guess the logits output; pick the first float output that looks like logits
            logits = None
            for o_meta, out in zip(dec_outputs_meta, outs):
                # typical name contains "logits"
                if "logits" in o_meta.name.lower():
                    logits = out
                    break
            if logits is None:
                # fallback to first float array with last dim = vocab_size
                for out in outs:
                    if out.dtype == np.float32 and out.ndim >= 2:
                        logits = out
                        break
            if logits is None:
                raise RuntimeError("Couldn't find logits output from decoder outputs; please inspect model outputs.")

            # take last token logits -> new token
            next_token = int(np.argmax(logits[0, -1, :]))
            generated.append(next_token)
            if next_token == tokenizer.eos_token_id:
                break

        decoded = tokenizer.decode(generated, skip_special_tokens=True)
        return decoded

    else:
        # KV-cache optimized decoding
        # Find which inputs are: input_ids, encoder_hidden_states, and list of past kv inputs
        # Build a map of input name -> meta
        name2meta = {m.name: m for m in dec_inputs_meta}
        # find input_ids input name
        input_ids_name = None
        for n in dec_input_names:
            if "input_ids" in n.lower() or "input" in n.lower() and name2meta[n].type.endswith("tensor(int64)"):
                input_ids_name = n
                break
        if input_ids_name is None:
            # fallback to first int64 input
            for m in dec_inputs_meta:
                if m.type.endswith("tensor(int64)"):
                    input_ids_name = m.name
                    break
        # find encoder_hidden_states input name
        enc_h_name = None
        for n in dec_input_names:
            if "encoder" in n.lower() and ("hidden" in n.lower() or "states" in n.lower()):
                enc_h_name = n
                break
        if enc_h_name is None:
            # fallback: any float input with rank 3 or 4
            for m in dec_inputs_meta:
                if m.type.endswith("tensor(float)"):
                    enc_h_name = m.name
                    break

        # collect past kv input names (those that contain "past", "key", "value", "cache", "kv")
        past_input_names = [n for n in dec_input_names if ("past" in n.lower() or "key" in n.lower() or "value" in n.lower() or "cache" in n.lower() or "kv" in n.lower()) and n != enc_h_name]
        print("Detected past kv input names:", past_input_names)

        # initialize past kvs as zeros according to their shapes (replace dynamic dims by 0 for sequence dims)
        past_values = {}
        for name in past_input_names:
            meta = name2meta[name]
            shape = []
            for d in meta.shape:
                if isinstance(d, str) or d is None or d < 0:
                    # dynamic -> try to interpret: if dim likely sequence length, set to 0 initially; else 1
                    shape.append(0)
                else:
                    shape.append(int(d))
            # ensure batch dim = batch_size if first dim looks like batch
            if len(shape) >= 1 and shape[0] in (0,1):
                shape[0] = batch_size
            # For 0-length sequence dims ONNX may expect 0 or 1 depending on export; if 0 not allowed, set to 1
            shape = [int(max(0, s)) for s in shape]
            # create zeros
            arr = np.zeros(shape, dtype=np.float32)
            past_values[name] = arr

        # prepare tokens
        generated = [start_token_id]
        # we'll update past_values each step by reading decoder outputs that look like 'present' or 'past'
        for step in range(MAX_OUTPUT_TOKENS):
            input_ids = np.array([[generated[-1]]], dtype=np.int64)  # feed only the last token for cached decoders
            feed = {}
            if input_ids_name is not None:
                feed[input_ids_name] = input_ids
            if enc_h_name is not None:
                feed[enc_h_name] = encoder_hidden_states
            # add past kvs to feed (if required)
            for k, v in past_values.items():
                feed[k] = v

            outs = dec_sess.run(None, feed)

            # get logits (same heuristics as above)
            logits = None
            for o_meta, out in zip(dec_outputs_meta, outs):
                if "logits" in o_meta.name.lower():
                    logits = out
                    break
            if logits is None:
                for out in outs:
                    if out.dtype == np.float32 and out.ndim >= 2:
                        logits = out
                        break
            if logits is None:
                raise RuntimeError("Couldn't find logits output from decoder outputs; please inspect model outputs.")

            next_token = int(np.argmax(logits[0, -1, :]))
            generated.append(next_token)

            # attempt to capture 'present' outputs and map back to past_values for next step
            # find any out names that contain 'present' or 'past' or 'key' and pack them back
            for o_meta, out in zip(dec_outputs_meta, outs):
                oname = o_meta.name.lower()
                if ("present" in oname) or ("past" in oname) or ("key" in oname and "value" in oname) or ("kv" in oname) or ("cache" in oname):
                    # heuristically try to find matching input name to write to
                    # many exports use identical names for present and past (prefix present_ vs past_)
                    candidate_in = None
                    cand = oname.replace("present", "past")
                    for nin in past_input_names:
                        if cand in nin.lower() or nin.lower() in oname:
                            candidate_in = nin
                            break
                    if candidate_in is None and len(past_input_names) == 1:
                        candidate_in = past_input_names[0]
                    if candidate_in:
                        past_values[candidate_in] = out.astype(np.float32)

            if next_token == tokenizer.eos_token_id:
                break

        decoded = tokenizer.decode(generated, skip_special_tokens=True)
        return decoded

if __name__ == "__main__":
    result = run_pipeline(ENCODER_ONNX, DECODER_ONNX, AUDIO_FILE, device=DEVICE)
    print("Transcription:\n", result)

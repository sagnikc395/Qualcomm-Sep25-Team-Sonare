# Sign â†” Speech (Offline) â€” Edge AI Demo

Fully on-device, privacy-preserving **Sign â†’ Speech** and **Speech â†’ Sign** translator built for the Qualcomm Hackathon September 2025. **No internet required** after installation.

- **Platform:** Cross-platform (Electron + Python)
- **UI:** React + TypeScript + Electron
- **On-device CV:** MediaPipe Tasks (Hands) in renderer
- **Classification:** Real-time gesture recognition with temporal stabilization
- **ASR:** whisper.cpp for speech-to-text
- **TTS:** Browser speechSynthesis API
- **Backend:** FastAPI server for keypoint processing

---

## ğŸ¯ Project Overview

This project demonstrates a complete **offline sign language translation system** with two main flows:

1. **Sign â†’ Speech**: Camera captures hand gestures â†’ MediaPipe extracts keypoints â†’ Gesture classifier â†’ Text output â†’ TTS
2. **Speech â†’ Sign**: Microphone captures speech â†’ whisper.cpp ASR â†’ Text processing â†’ Sign language video playback

**Key Features:**

- âœ… **100% Offline** - No internet required after setup
- âœ… **Real-time Processing** - ~15-20 FPS hand detection
- âœ… **Privacy-First** - All processing happens locally
- âœ… **Cross-Platform** - Works on Windows, macOS, Linux
- âœ… **Modular Architecture** - Separate UI and backend components

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Frontend (Electron)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  React UI  â”‚  Camera Capture  â”‚  MediaPipe Hands  â”‚  Gesture   â”‚
â”‚            â”‚                  â”‚  Keypoint Extract â”‚  Classifierâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Backend (FastAPI)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Keypoint Queue  â”‚  Temporal Logic  â”‚  ASR (whisper.cpp)  â”‚ TTSâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Breakdown

- **`src/ui/`** - Electron + React frontend with camera capture and gesture recognition
- **`src/server/`** - FastAPI backend for keypoint processing and queue management
- **`src/speech_to_sign/`** - Speech-to-sign pipeline with whisper.cpp integration and it also contains the dataset.
- **`src/Video_Sampling/`** - Sign language video dataset and processing utilities

---

## ğŸš€ Quick Start

### Prerequisites

- **Node.js â‰¥ 18** (LTS)
- **Python â‰¥ 3.13**
- **uv** (Python package manager)
- **Camera + Microphone** access

### Installation

1. **Clone and setup Python environment:**

```bash
git clone https://github.com/abhishek9909/Qualcomm-Sep25-Team-Sonare.git
cd Qualcomm-Sep25-Team-Sonare

# Install uv if not already installed
brew install uv  # macOS
# or: curl -LsSf https://astral.sh/uv/install.sh | sh  # Linux

# Setup Python environment
uv venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uv sync
```

2. **Setup UI dependencies:**

```bash
cd src/ui
npm install
```

3. **Download required models:**

```bash
MediaPipe hand landmarker model (place in src/ui/public/assets/mediapipe/)
whisper.cpp model (place in src/speech_to_sign/whisper.cpp/models/)
```

### Running the Application

**Option 1: Full Stack (Recommended)**

```bash
# Terminal 1: Start backend server
cd src/server
uv run uvicorn main:app --reload --host 0.0.0.0 --port 8000

# Terminal 2: Start UI
cd src/ui
npm start
```

**Option 2: Speech-to-Sign Pipeline Only**

- [ ] **npm run package** - creates an installer which for the app.
- [ ] **python inference_basic.py** - command to run the lemmatization and word-mapping and video-sticthing server, running completely on local.
- [ ] **python inference_whisper.py** - command to run the whisper model on local.

---

## To run the pipeline in the demo:

- [ ] **npm run package** - creates an installer which for the app.
- [ ] **python inference_basic.py** - command to run the lemmatization and word-mapping and video-sticthing server, running completely on local.
- [ ] **python inference_whisper.py** - command to run the whisper model on local.

## ğŸ“ Project Structure

```
Qualcomm-Sep25-Team-Sonare/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ui/                          # Electron + React Frontend
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ main/               # Electron main process
â”‚   â”‚   â”‚   â”œâ”€â”€ renderer/           # React UI components
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ services/       # MediaPipe, GestureClassifier
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ CameraScreen.tsx
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ App.tsx
â”‚   â”‚   â”‚   â””â”€â”€ preload/            # IPC bridge
â”‚   â”‚   â”œâ”€â”€ public/assets/mediapipe/ # MediaPipe models & WASM
â”‚   â”‚   â””â”€â”€ package.json
â”‚   â”‚
â”‚   â”œâ”€â”€ server/                      # FastAPI Backend
â”‚   â”‚   â”œâ”€â”€ main.py                 # Keypoint processing API
â”‚   â”‚   â””â”€â”€ justfile               # Development commands
â”‚   â”‚
â”‚   â”œâ”€â”€ speech_to_sign/             # Speech-to-Sign Pipeline
â”‚       â”œâ”€â”€ run_all.sh             # Main pipeline script
â”‚       â”œâ”€â”€ clean_transcript.py    # Text preprocessing
â”‚       â”œâ”€â”€ glossify_transcript.py # Text-to-gloss mapping
â”‚       â”œâ”€â”€ stream_queue_assets.py # Video queue management
â”‚       â”œâ”€â”€ lexicons.json          # Sign language vocabulary
â”‚       â”œâ”€â”€ videos/                # Sign language video assets
â”‚       â””â”€â”€ whisper.cpp/           # whisper.cpp integration
â”‚
â”œâ”€â”€ code_samples/                   # Development notes & examples
â”œâ”€â”€ pyproject.toml                 # Python dependencies
â””â”€â”€ README.md
```

---

## ğŸ® Usage

### Sign â†’ Speech Flow

1. **Start the application** (see Quick Start)
2. **Allow camera permissions** when prompted
3. **Perform sign language gestures** in front of the camera
4. **View recognized gestures** in real-time with confidence scores
5. **Listen to text-to-speech** output (if enabled)

**Supported Gestures:**

- Basic: Fist, Open Hand, Peace Sign, Thumbs Up/Down
- Advanced: Pointing, OK Sign, Wave
- Custom: Extensible gesture recognition system

### Speech â†’ Sign Flow

1. **Switch to Speech-to-Sign mode**
2. **Hold Space** to start recording speech
3. **Speak clearly** and release Space to stop
4. **Watch sign language videos** corresponding to recognized words

**Supported Vocabulary:** 90+ sign language words including:

- Greetings: HI, HELLO, GOOD MORNING
- Actions: EAT, DRINK, GO, COME, SIT, STAND
- People: I, YOU, HE, SHE, FAMILY, TEACHER
- Objects: FOOD, WATER, COFFEE, BOOK
- And many more...

---

## âš™ï¸ Configuration

### Environment Variables

Create `.env` files as needed:

**Backend (.env):**

```bash
# Server settings
HOST=0.0.0.0
PORT=8000
LOG_LEVEL=info

# MediaPipe settings
MEDIAPIPE_WASM_DIR=src/ui/public/assets/mediapipe/wasm
MEDIAPIPE_TASK_PATH=src/ui/public/assets/mediapipe/hand_landmarker.task

# Whisper settings
WHISPER_MODEL_PATH=src/speech_to_sign/whisper.cpp/models/ggml-tiny.en.bin
WHISPER_BINARY_PATH=src/speech_to_sign/whisper.cpp/build/bin/whisper-stream
```

**Frontend (.env):**

```bash
# API endpoints
REACT_APP_API_URL=http://localhost:8000
REACT_APP_WS_URL=ws://localhost:8000/ws

# MediaPipe settings
REACT_APP_MEDIAPIPE_WASM_URL=/assets/mediapipe/wasm
REACT_APP_MEDIAPIPE_MODEL_URL=/assets/mediapipe/hand_landmarker.task
```

### Performance Tuning

**For better performance:**

- Use GPU acceleration for MediaPipe (enabled by default)
- Adjust `minHandDetectionConfidence` in `HandLandmarkerService.ts`
- Modify gesture stability thresholds in `GestureClassifier.ts`
- Tune whisper.cpp parameters in `run_all.sh`

---

## ğŸ”§ Development

### Available Scripts

**Python Backend:**

```bash
cd src/server
just dev          # Start development server
just prod         # Start production server
just health       # Check server health
just stats        # Get processing statistics
```

**React Frontend:**

```bash
cd src/ui
npm start         # Start development server
npm run build     # Build for production
npm run package   # Package Electron app
npm test          # Run tests
npm run lint      # Lint code
```

**Speech-to-Sign Pipeline:**

- [ ] **npm run package** - creates an installer which for the app.
- [ ] **python inference_basic.py** - command to run the lemmatization and word-mapping and video-sticthing server, running completely on local.
- [ ] **python inference_whisper.py** - command to run the whisper model on local.

### Adding New Gestures

1. **Update gesture detection** in `src/ui/src/renderer/services/GestureClassifier.ts`
2. **Add gesture patterns** in the `detectGesture()` method
3. **Update confidence calculations** for new gestures
4. **Test with real hand data** and adjust thresholds

### Adding New Sign Language Words

1. **Add video assets** to `src/speech_to_sign/videos/`
2. **Update vocabulary** in `src/speech_to_sign/lexicons.json`
3. **Test speech recognition** with new words
4. **Verify video playback** in the UI

---

## ğŸ“Š Performance Metrics

**Current Performance:**

- **Hand Detection:** ~15-20 FPS
- **Gesture Recognition:** ~200ms latency
- **Speech Recognition:** ~650ms latency (whisper.cpp tiny)
- **Memory Usage:** ~430MB (Electron + Python)
- **CPU Usage:** ~22% (on modern hardware)

**Optimization Opportunities:**

- ONNX Runtime integration for faster inference
- WebAssembly optimizations for MediaPipe
- Model quantization for smaller memory footprint
- Multi-threading for parallel processing

---

## ğŸ› Troubleshooting

### Common Issues

**Camera not working:**

- Check browser/OS camera permissions
- Verify camera is not used by other applications
- Try different camera sources (0, 1, 2...)

**MediaPipe not loading:**

- Ensure WASM files are in correct location
- Check browser console for CORS errors
- Verify model file paths in configuration

**Speech recognition failing:**

- Check microphone permissions
- Verify whisper.cpp model is downloaded
- Ensure audio input is working

**Gesture recognition inaccurate:**

- Improve lighting conditions
- Keep hands in frame and well-lit
- Adjust confidence thresholds
- Check for background interference

### Debug Mode

Enable debug logging:

```bash
# Backend
export LOG_LEVEL=debug
uv run uvicorn main:app --reload

# Frontend
npm start -- --debug
```

---

## ğŸ¤ Contributing

1. **Fork the repository**
2. **Create a feature branch** (`git checkout -b feature/amazing-feature`)
3. **Commit your changes** (`git commit -m 'Add amazing feature'`)
4. **Push to the branch** (`git push origin feature/amazing-feature`)
5. **Open a Pull Request**

### Development Guidelines

- Follow TypeScript best practices
- Add tests for new features
- Update documentation for API changes
- Ensure cross-platform compatibility
- Maintain performance benchmarks

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¥ Team

**Qualcomm Hackathon September 2025 - Team Sonare**

- **Armaan Sandhu** (apsandhu@umass.edu)
- **Archana Yadav** (archanayadav@umass.edu)
- **Abhishek Mishra** (abhishekmish@umass.edu)
- **Sagnik Chatterjee** (sagnikchatte@umass.edu)
- **Deva Anand** (devaanad@umass.edu)

---

## ğŸ™ Acknowledgments

- **MediaPipe** - Hand landmark detection
- **whisper.cpp** - Speech recognition
- **Electron** - Desktop app framework
- **React** - UI framework
- **FastAPI** - Backend framework
- **Qualcomm** - Hackathon platform and support

---

## ğŸ“ˆ Future Roadmap

- [ ] **ONNX Runtime integration** for faster inference
- [ ] **Multi-language support** (ASL, BSL, etc.)
- [ ] **Avatar-based sign rendering** (replace video files)
- [ ] **Real-time collaboration** features
- [ ] **Mobile app** version
- [ ] **Cloud deployment** options
- [ ] **Advanced gesture recognition** with deep learning
- [ ] **Sign language synthesis** from text

---

**Built with â¤ï¸ for the Qualcomm Hackathon September 2025**
